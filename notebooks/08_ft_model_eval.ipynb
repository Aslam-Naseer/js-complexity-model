{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2795be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade bitsandbytes trl\n",
    "# !wget -q https://raw.githubusercontent.com/aslam-naseer/js-complexity-model/master/notebooks/utils/evaluator.py -O evaluator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb27b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from evaluator import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f810f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "MODEL = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "PROJECT_NAME = \"complexity\"\n",
    "HF_USER = \"aslam-naseer\"\n",
    "\n",
    "DATA_USER = \"aslam-naseer\"\n",
    "DATASET_NAME = f\"{DATA_USER}/js-function-complexity-messages\"\n",
    "\n",
    "RUN_NAME = \"2026-01-08_05.47.09\"\n",
    "\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
    "\n",
    "\n",
    "# Hyper-parameters - QLoRA\n",
    "\n",
    "QUANT_4_BIT = True\n",
    "capability = torch.cuda.get_device_capability()\n",
    "use_bf16 = capability[0] >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b5c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "test = dataset['test']\n",
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235282fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tokenizer and the Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load the fine-tuned model with PEFT\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, HUB_MODEL_NAME)\n",
    "\n",
    "\n",
    "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea74efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(item):\n",
    "    inputs = tokenizer(item[\"prompt\"],return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = fine_tuned_model.generate(**inputs, max_new_tokens=8)\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = output_ids[0, prompt_len:]\n",
    "    return tokenizer.decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2073a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "evaluate(model_predict, test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
